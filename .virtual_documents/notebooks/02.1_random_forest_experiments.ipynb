import sys
sys.path.append('..') 


# load data
from utils import load_train_val_data, load_test_data

X_train, X_val, y_train, y_val = load_train_val_data()
X_test, y_test = load_test_data()


# initialize trainer
from utils import ModelTrainer

trainer = ModelTrainer(
    cv_folds=5,       
    scoring='accuracy',
    n_trials=75,      
    random_state=42,
    verbose=True
)


from utils import ModelEvaluator

evaluator = ModelEvaluator()





# QUICK FEATURE SELECTION TEST - Add this cell instead for faster results

from models import RandomForestModel
import numpy as np
from sklearn.feature_selection import SelectKBest, f_classif, SelectFromModel, chi2, mutual_info_classif
from sklearn.ensemble import RandomForestClassifier

print("‚ö° QUICK FEATURE SELECTION TEST")
print("=" * 40)

techniques = [
    # Chi-squared: THE classic for text classification
    ("Chi2_500", SelectKBest(chi2, k=500)),
    ("Chi2_750", SelectKBest(chi2, k=750)),
    ("Chi2_1000", SelectKBest(chi2, k=1000)),
    ("Chi2_1500", SelectKBest(chi2, k=1500)),
    
    # Mutual Information: Captures complex term relationships
    ("MutualInfo_500", SelectKBest(mutual_info_classif, k=500)),
    ("MutualInfo_750", SelectKBest(mutual_info_classif, k=750)),
    ("MutualInfo_1000", SelectKBest(mutual_info_classif, k=1000)),

    # Statistical selection - likely to work well with TF-IDF
    ("F-Score_500", SelectKBest(f_classif, k=500)),
    ("F-Score_1000", SelectKBest(f_classif, k=1000)),
    ("F-Score_1500", SelectKBest(f_classif, k=1500)),


    # RF importance - uses same algorithm as your model
    ("RF_Import_1000", SelectFromModel(
        RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1), 
        max_features=1000)
    )
]
    

results = []

for name, selector in techniques:
    print(f"\nüß™ Testing {name}...")
    
    # Select features
    X_train_sel = selector.fit_transform(X_train, y_train)
    X_val_sel = selector.transform(X_val)
    X_test_sel = selector.transform(X_test)
    n_features = X_train_sel.shape[1]
    
    print(f"   Selected: {n_features} features ({n_features/X_train.shape[1]*100:.1f}%)")
    
    # Quick test with simple RF
    rf_test = RandomForestModel(n_estimators=167, max_depth=10, min_samples_split = 24, min_samples_leaf = 8, max_features = 0.6, random_state=42)
    rf_test.name = f"Test_{name}"
    
    test_results = trainer.train_model(
        model=rf_test,
        X_train=X_train_sel,
        y_train=y_train, 
        X_val=X_val_sel,
        y_val=y_val,
        optimize=False
    )
    
    val_acc = test_results['val_accuracy']
    print(f"   üéØ Val accuracy: {val_acc:.4f}")
    
    results.append({
        'name': name,
        'n_features': n_features,
        'val_acc': val_acc,
        'selector': selector,
        'X_train_sel': X_train_sel,
        'X_val_sel': X_val_sel,
        'X_test_sel': X_test_sel
    })

# Find best result
best = max(results, key=lambda x: x['val_acc'])
print(f"\nüèÜ BEST QUICK TEST: {best['name']}")
print(f"   Features: {best['n_features']}")
print(f"   Accuracy: {best['val_acc']:.4f}")


import pickle
import os

# Create directory if it doesn't exist
data_path = "../data/processed/feature_extracted/"
os.makedirs(data_path, exist_ok=True)

file_name = f"extracted_features"

# Save best results
with open(f'{data_path}{file_name}.pkl', 'wb') as f:
    pickle.dump({
        'best': best,
        'all_results': results
    }, f)
    
print(f"üíæ Results saved to {file_name}.pkl")








import pickle
def load_best_features(filename='best_feature_results.pkl'):
   with open(filename, 'rb') as f:
       data = pickle.load(f)
   
   best = data['best']
   all_results = data['all_results']
   
   print(f"üìÅ Loaded: {best['name']} - {best['val_acc']:.4f} accuracy")
   print(f"   Features: {best['n_features']}")
   
   return best, all_results

# Create directory if it doesn't exist
data_path = "../data/processed/feature_extracted/"

file_name = f"extracted_features"
best, all_results = load_best_features(filename=f"{data_path}{file_name}.pkl")

selector_name = best["name"]
X_train = best['X_train_sel']
X_val = best['X_val_sel']
X_test = best['X_test_sel']






# Training function to standardise outputs for all experiments

def train(model, model_name, trainer, X_train, y_train, X_val, y_val, optimize = True, param_space=None):
    
    model.name = model_name

    print(f"üéØ Training {model_name} with Bayesian Optimization...")
    print(f"   Features: {best['X_train_sel'].shape[1]} (chi-squared selected)")
    print(f"   Parameter space: {len(param_space)} hyperparameters" if param_space is not None else "   default")
    print(f"   Optimization trials: {trainer.n_trials}")

    # Train with comprehensive hyperparameter optimization
    training_results = trainer.train_model(
        model=model,
        X_train=X_train,
        y_train=y_train,
        X_val=X_val,
        y_val=y_val,
        param_space=param_space,
        optimize=optimize 
    )

    print(f"\nüîç {model_name} Training Results:")
    if training_results['optimization']:
        opt = training_results['optimization']
        print(f"   Optimization completed: {opt['n_trials']} trials in {opt['optimization_time']:.1f}s")
        print(f"   Best CV Score: {opt['best_score']:.4f}")
        print(f"   Best Parameters: {opt['best_params']}")

    print(f"   Final CV Score: {training_results['cv_mean']:.4f} ¬± {training_results['cv_std']:.4f}")
    print(f"   Train Accuracy: {training_results['train_accuracy']:.4f}")
    print(f"   Val Accuracy: {training_results['val_accuracy']:.4f}")
    print(f"   Overfitting Gap: {training_results['train_accuracy'] - training_results['val_accuracy']:.4f}")

    return training_results


# evaluation function to standardise model evaluation

def evaluate(model, model_name, evaluator, X_train, y_train, X_val, y_val, X_test, y_test):

    # Comprehensive evaluation with test set
    print(f"\nüìä Running comprehensive evaluation...")
    eval_results = evaluator.evaluate_model(
        model=model,
        X_train=X_train,
        y_train=y_train,
        X_val=X_val, 
        y_val=y_val,
        X_test=X_test,
        y_test=y_test,
        model_name=model_name
    )

    # Print detailed evaluation report
    evaluator.print_detailed_report(model_name)

    # Plot overfitting analysis across models
    evaluator.plot_overfitting_analysis()

    # Plot confusion matrix
    evaluator.plot_confusion_matrix(model_name)

    return eval_results


import os
import pickle

# save experiment function to standardise experiment saving
experiment_path = "../models/trained/RandomForest/"

def save_experiment(data, filename, dir=experiment_path):
    os.makedirs(dir, exist_ok=True)  
    with open(f"{dir}{filename}", "wb") as file:
        pickle.dump(data, file)
    print(f"Experiment saved to {dir}{filename}")
    









from models import RandomForestModel

# Create baseline RandomForest with default parameters
baseline_rf = RandomForestModel(
    n_estimators=100,
    max_depth=None,
    min_samples_leaf=1,
    min_samples_split=2,
    max_features='sqrt',
    random_state=42
)

baseline_rf_name = f"Baseline_RF_{selector_name}"

baseline_train_results = train(model = baseline_rf, 
      model_name = baseline_rf_name, 
      trainer = trainer,
      X_train = X_train, 
      y_train = y_train, 
      X_val = X_val, 
      y_val = y_val, 
      optimize = False)






# evaluation function to standardise model evaluation
baseline_eval_results = evaluate(
    model = baseline_rf,
    model_name = baseline_rf_name,
    evaluator = evaluator,
    X_train = X_train,
    y_train = y_train,
    X_val = X_val,
    y_val = y_val,
    X_test = X_test,
    y_test = y_test
)

experiment_name = baseline_rf_name

baseline_experiment = {
    "experiment_name" : experiment_name,
    "model" : baseline_rf,
    "train_results" : baseline_train_results,
    "eval_results" : baseline_eval_results
}

save_experiment(baseline_experiment, f"{experiment_name}.pkl")








# ENHANCED VERSION with additional anti-overfitting parameters
anti_overfit_space = {
    # Core tree parameters (your original)
    'n_estimators': (200, 500),
    'max_depth': (3, 8),
    'min_samples_split': (20, 100),
    'min_samples_leaf': (15, 50),
    'max_features': ['sqrt', 'log2', 0.2, 0.3, 0.4],
    'max_samples': (0.6, 0.9),
    'min_impurity_decrease': (0.0, 0.01), 
    'min_weight_fraction_leaf': (0.0, 0.05),
    'ccp_alpha': (0.0, 0.1),
    'bootstrap': [True],
    'oob_score': [True],
    'criterion': ['gini', 'entropy'],
}

# Create baseline RandomForest with default parameters
anti_overfit_rf = RandomForestModel(random_state=42)

anti_overfit_name = f"AntiOverfit_RF_{selector_name}"

anti_overfit_train_results = train(
        model = anti_overfit_rf, 
        model_name = anti_overfit_rf, 
        trainer = trainer,
        X_train = X_train, 
        y_train = y_train, 
        X_val = X_val, 
        y_val = y_val, 
        optimize = True,
        param_space= anti_overfit_space
)






# evaluation function to standardise model evaluation
anti_overfit_eval_results = evaluate(
    model = anti_overfit_rf,
    model_name = anti_overfit_name,
    evaluator = evaluator,
    X_train = X_train,
    y_train = y_train,
    X_val = X_val,
    y_val = y_val,
    X_test = X_test,
    y_test = y_test
)

experiment_name = anti_overfit_name

anti_overfit_experiment = {
    "experiment_name" : experiment_name,
    "model" : anti_overfit_rf,
    "train_results" : anti_overfit_train_results,
    "eval_results" : anti_overfit_eval_results
}

save_experiment(anti_overfit_experiment, f"{experiment_name}.pkl")








param_space = {
    'n_estimators': (400, 1000),        # Increased for better performance
    'max_depth': (10, 20),              # Deeper for text classification
    'min_samples_split': (5, 25),       # More conservative to prevent overfitting
    'min_samples_leaf': (2, 10),        # Increased minimum from 1 to 2
    'max_features': [0.3, 0.5, 0.7, 'sqrt', 'log2'],  # Added 'log2', removed 0.8
    'class_weight': [None, 'balanced'],  # Important for fairness
    'random_state': [42]                 # Consistency
}

# Create baseline RandomForest with default parameters
rf = RandomForestModel(random_state=42)

rf_name = f"HighPerformance_RF_{selector_name}"

rf_train_results = train(
        model = rf, 
        model_name = rf_name, 
        trainer = trainer,
        X_train = X_train, 
        y_train = y_train, 
        X_val = X_val, 
        y_val = y_val, 
        optimize = True,
        param_space= param_space
)





# evaluation function to standardise model evaluation
rf_eval_results = evaluate(
    model = rf,
    model_name = rf_name,
    evaluator = evaluator,
    X_train = X_train,
    y_train = y_train,
    X_val = X_val,
    y_val = y_val,
    X_test = X_test,
    y_test = y_test
)

experiment_name = rf_name

rf_experiment = {
    "experiment_name" : experiment_name,
    "model" : rf,
    "train_results" : rf_train_results,
    "eval_results" : rf_eval_results
}

save_experiment(rf_experiment, f"{experiment_name}.pkl")








balanced_space = {
    'n_estimators': (150, 400),                    # Increased for better stability
    'max_depth': (8, 15),                          # Deeper for text patterns
    'min_samples_split': (8, 30),                  # More flexible splitting
    'min_samples_leaf': (3, 15),                   # Reduced for better learning
    'max_features': ['sqrt', 'log2', 0.3, 0.5, 0.7],  # More granular options
    'criterion': ['gini', 'entropy'],              # Keep both criteria
    'class_weight': [None, 'balanced'],            # Added for fairness
    'random_state': [42]                           # Added for consistency
}

# Create baseline RandomForest with default parameters
rf = RandomForestModel(random_state=42)

rf_name = f"Balanced_RF_{selector_name}"

rf_train_results = train(
        model = rf, 
        model_name = rf_name, 
        trainer = trainer,
        X_train = X_train, 
        y_train = y_train, 
        X_val = X_val, 
        y_val = y_val, 
        optimize = True,
        param_space= param_space
)





# evaluation function to standardise model evaluation
rf_eval_results = evaluate(
    model = rf,
    model_name = rf_name,
    evaluator = evaluator,
    X_train = X_train,
    y_train = y_train,
    X_val = X_val,
    y_val = y_val,
    X_test = X_test,
    y_test = y_test
)

experiment_name = rf_name

rf_experiment = {
    "experiment_name" : experiment_name,
    "model" : rf,
    "train_results" : rf_train_results,
    "eval_results" : rf_eval_results
}

save_experiment(rf_experiment, f"{experiment_name}.pkl")





# Compare all models
comparison_df = evaluator.compare_models()



