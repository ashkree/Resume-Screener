{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70162555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "969fa919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Loading train/val data...\n",
      "âœ… Data loaded:\n",
      "   X_train: (2100, 5000)\n",
      "   X_val: (600, 5000)\n",
      "   y_train: 2100 samples\n",
      "   y_val: 600 samples\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "from utils import load_train_val_data\n",
    "\n",
    "X_train, X_val, y_train, y_val = load_train_val_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fd99a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_pipeline import ModelTrainer\n",
    "\n",
    "# 1. Create trainer\n",
    "trainer = ModelTrainer(\n",
    "    cv_folds=8,\n",
    "    scoring='f1_weighted',\n",
    "    n_trials=20,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e8e6cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ QUICK FEATURE SELECTION TEST\n",
      "========================================\n",
      "\n",
      "ğŸ§ª Testing F-Score_500...\n",
      "   Selected: 500 features (10.0%)\n",
      "ğŸš€ Training Test_F-Score_500...\n",
      "ğŸ”§ Fitting Test_F-Score_500...\n",
      "âœ… Test_F-Score_500 fitted in 5.20 seconds\n",
      "ğŸ”„ Running 8-fold cross-validation...\n",
      "âœ… Test_F-Score_500 completed in 10.9s\n",
      "   CV: 0.5550 Â± 0.0259\n",
      "   Train: 0.8048\n",
      "   Val: 0.5883 (gap: 0.2164)\n",
      "   ğŸ¯ Val accuracy: 0.5883\n",
      "\n",
      "ğŸ§ª Testing F-Score_1000...\n",
      "   Selected: 1000 features (20.0%)\n",
      "ğŸš€ Training Test_F-Score_1000...\n",
      "ğŸ”§ Fitting Test_F-Score_1000...\n",
      "âœ… Test_F-Score_1000 fitted in 9.96 seconds\n",
      "ğŸ”„ Running 8-fold cross-validation...\n",
      "âœ… Test_F-Score_1000 completed in 20.9s\n",
      "   CV: 0.5573 Â± 0.0356\n",
      "   Train: 0.8362\n",
      "   Val: 0.6000 (gap: 0.2362)\n",
      "   ğŸ¯ Val accuracy: 0.6000\n",
      "\n",
      "ğŸ§ª Testing F-Score_1500...\n",
      "   Selected: 1500 features (30.0%)\n",
      "ğŸš€ Training Test_F-Score_1500...\n",
      "ğŸ”§ Fitting Test_F-Score_1500...\n",
      "âœ… Test_F-Score_1500 fitted in 14.87 seconds\n",
      "ğŸ”„ Running 8-fold cross-validation...\n",
      "âœ… Test_F-Score_1500 completed in 31.2s\n",
      "   CV: 0.5763 Â± 0.0285\n",
      "   Train: 0.8386\n",
      "   Val: 0.6100 (gap: 0.2286)\n",
      "   ğŸ¯ Val accuracy: 0.6100\n",
      "\n",
      "ğŸ§ª Testing RF_Import_1000...\n",
      "   Selected: 1000 features (20.0%)\n",
      "ğŸš€ Training Test_RF_Import_1000...\n",
      "ğŸ”§ Fitting Test_RF_Import_1000...\n",
      "âœ… Test_RF_Import_1000 fitted in 23.34 seconds\n",
      "ğŸ”„ Running 8-fold cross-validation...\n",
      "âœ… Test_RF_Import_1000 completed in 49.1s\n",
      "   CV: 0.5710 Â± 0.0323\n",
      "   Train: 0.8690\n",
      "   Val: 0.5950 (gap: 0.2740)\n",
      "   ğŸ¯ Val accuracy: 0.5950\n",
      "\n",
      "ğŸ† BEST QUICK TEST: F-Score_1500\n",
      "   Features: 1500\n",
      "   Accuracy: 0.6100\n"
     ]
    }
   ],
   "source": [
    "# QUICK FEATURE SELECTION TEST - Add this cell instead for faster results\n",
    "\n",
    "from models import RandomForestModel\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"âš¡ QUICK FEATURE SELECTION TEST\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test just the most promising approaches\n",
    "quick_tests = [\n",
    "    # Statistical selection - likely to work well with TF-IDF\n",
    "    (\"F-Score_500\", SelectKBest(f_classif, k=500)),\n",
    "    (\"F-Score_1000\", SelectKBest(f_classif, k=1000)),\n",
    "    (\"F-Score_1500\", SelectKBest(f_classif, k=1500)),\n",
    "    \n",
    "    # RF importance - uses same algorithm as your model\n",
    "    (\"RF_Import_1000\", SelectFromModel(\n",
    "        RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1), \n",
    "        max_features=1000)\n",
    "    ),\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, selector in quick_tests:\n",
    "    print(f\"\\nğŸ§ª Testing {name}...\")\n",
    "    \n",
    "    # Select features\n",
    "    X_train_sel = selector.fit_transform(X_train, y_train)\n",
    "    X_val_sel = selector.transform(X_val)\n",
    "    n_features = X_train_sel.shape[1]\n",
    "    \n",
    "    print(f\"   Selected: {n_features} features ({n_features/X_train.shape[1]*100:.1f}%)\")\n",
    "    \n",
    "    # Quick test with simple RF\n",
    "    rf_test = RandomForestModel(n_estimators=167, max_depth=10, min_samples_split = 24, min_samples_leaf = 8, max_features = 0.6, random_state=42)\n",
    "    rf_test.name = f\"Test_{name}\"\n",
    "    \n",
    "    test_results = trainer.train_model(\n",
    "        model=rf_test,\n",
    "        X_train=X_train_sel,\n",
    "        y_train=y_train, \n",
    "        X_val=X_val_sel,\n",
    "        y_val=y_val,\n",
    "        optimize=False\n",
    "    )\n",
    "    \n",
    "    val_acc = test_results['val_accuracy']\n",
    "    print(f\"   ğŸ¯ Val accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    results.append({\n",
    "        'name': name,\n",
    "        'n_features': n_features,\n",
    "        'val_acc': val_acc,\n",
    "        'selector': selector,\n",
    "        'X_train_sel': X_train_sel,\n",
    "        'X_val_sel': X_val_sel\n",
    "    })\n",
    "\n",
    "# Find best result\n",
    "best = max(results, key=lambda x: x['val_acc'])\n",
    "print(f\"\\nğŸ† BEST QUICK TEST: {best['name']}\")\n",
    "print(f\"   Features: {best['n_features']}\")\n",
    "print(f\"   Accuracy: {best['val_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2ba845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-18 23:44:22,802] A new study created in memory with name: no-name-1ae3f20b-4540-4670-aaf9-58b65d3b6de8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Optimizing best feature selection...\n",
      "ğŸš€ Training Optimized_F-Score_1500...\n",
      "ğŸ” Optimizing Optimized_F-Score_1500 hyperparameters...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87a9ae6ca004719a7bafb13c671efc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-18 23:44:27,612] Trial 0 finished with value: 0.5599141950052853 and parameters: {'n_estimators': 487, 'max_depth': 15, 'min_samples_split': 12, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.5599141950052853.\n",
      "[I 2025-06-18 23:45:17,215] Trial 1 finished with value: 0.5773091028916159 and parameters: {'n_estimators': 601, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 8, 'max_features': 0.4}. Best is trial 1 with value: 0.5773091028916159.\n",
      "[I 2025-06-18 23:45:58,562] Trial 2 finished with value: 0.5780748304230525 and parameters: {'n_estimators': 452, 'max_depth': 12, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 0.4}. Best is trial 2 with value: 0.5780748304230525.\n",
      "[I 2025-06-18 23:47:36,656] Trial 3 finished with value: 0.5800157096838845 and parameters: {'n_estimators': 528, 'max_depth': 14, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': 0.8}. Best is trial 3 with value: 0.5800157096838845.\n",
      "[I 2025-06-18 23:48:35,316] Trial 4 finished with value: 0.5770588440025638 and parameters: {'n_estimators': 332, 'max_depth': 15, 'min_samples_split': 15, 'min_samples_leaf': 7, 'max_features': 0.8}. Best is trial 3 with value: 0.5800157096838845.\n",
      "[I 2025-06-18 23:49:13,642] Trial 5 finished with value: 0.5714272523495215 and parameters: {'n_estimators': 361, 'max_depth': 11, 'min_samples_split': 2, 'min_samples_leaf': 8, 'max_features': 0.6}. Best is trial 3 with value: 0.5800157096838845.\n",
      "[I 2025-06-18 23:49:49,968] Trial 6 finished with value: 0.5691955513515095 and parameters: {'n_estimators': 573, 'max_depth': 9, 'min_samples_split': 15, 'min_samples_leaf': 7, 'max_features': 0.4}. Best is trial 3 with value: 0.5800157096838845.\n",
      "[I 2025-06-18 23:50:37,155] Trial 7 finished with value: 0.5716933660948245 and parameters: {'n_estimators': 344, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 0.8}. Best is trial 3 with value: 0.5800157096838845.\n",
      "[I 2025-06-18 23:51:28,986] Trial 8 finished with value: 0.5818891664838555 and parameters: {'n_estimators': 440, 'max_depth': 12, 'min_samples_split': 3, 'min_samples_leaf': 7, 'max_features': 0.6}. Best is trial 8 with value: 0.5818891664838555.\n",
      "[I 2025-06-18 23:51:56,445] Trial 9 finished with value: 0.576051090450497 and parameters: {'n_estimators': 302, 'max_depth': 14, 'min_samples_split': 11, 'min_samples_leaf': 6, 'max_features': 0.4}. Best is trial 8 with value: 0.5818891664838555.\n",
      "[I 2025-06-18 23:53:43,442] Trial 10 finished with value: 0.5776931819827011 and parameters: {'n_estimators': 773, 'max_depth': 11, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 0.6}. Best is trial 8 with value: 0.5818891664838555.\n",
      "[I 2025-06-18 23:55:40,845] Trial 11 finished with value: 0.5883613531875014 and parameters: {'n_estimators': 667, 'max_depth': 13, 'min_samples_split': 6, 'min_samples_leaf': 5, 'max_features': 0.8}. Best is trial 11 with value: 0.5883613531875014.\n",
      "[I 2025-06-18 23:57:11,549] Trial 12 finished with value: 0.5798293851049845 and parameters: {'n_estimators': 695, 'max_depth': 12, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 0.6}. Best is trial 11 with value: 0.5883613531875014.\n",
      "[I 2025-06-18 23:57:16,259] Trial 13 finished with value: 0.5616475057774346 and parameters: {'n_estimators': 654, 'max_depth': 13, 'min_samples_split': 5, 'min_samples_leaf': 6, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.5883613531875014.\n",
      "[I 2025-06-18 23:58:18,673] Trial 14 finished with value: 0.5677544794322537 and parameters: {'n_estimators': 446, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 6, 'max_features': 0.8}. Best is trial 11 with value: 0.5883613531875014.\n",
      "[I 2025-06-19 00:00:10,740] Trial 15 finished with value: 0.5838849598602819 and parameters: {'n_estimators': 793, 'max_depth': 13, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 0.6}. Best is trial 11 with value: 0.5883613531875014.\n",
      "[I 2025-06-19 00:02:05,799] Trial 16 finished with value: 0.5824370480532282 and parameters: {'n_estimators': 800, 'max_depth': 13, 'min_samples_split': 10, 'min_samples_leaf': 3, 'max_features': 0.6}. Best is trial 11 with value: 0.5883613531875014.\n",
      "[I 2025-06-19 00:04:44,378] Trial 17 finished with value: 0.5827723038465771 and parameters: {'n_estimators': 740, 'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 0.8}. Best is trial 11 with value: 0.5883613531875014.\n",
      "[I 2025-06-19 00:04:47,829] Trial 18 finished with value: 0.5517290589834369 and parameters: {'n_estimators': 664, 'max_depth': 8, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.5883613531875014.\n",
      "[I 2025-06-19 00:06:37,552] Trial 19 finished with value: 0.5858091857436727 and parameters: {'n_estimators': 729, 'max_depth': 13, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 0.6}. Best is trial 11 with value: 0.5883613531875014.\n",
      "âœ… Optimization completed in 1334.8s\n",
      "   Best score: 0.5884\n",
      "   Best params: {'n_estimators': 667, 'max_depth': 13, 'min_samples_split': 6, 'min_samples_leaf': 5, 'max_features': 0.8}\n",
      "ğŸ”§ Fitting Optimized_F-Score_1500...\n",
      "âœ… Optimized_F-Score_1500 fitted in 107.03 seconds\n",
      "ğŸ”„ Running 8-fold cross-validation...\n",
      "âœ… Optimized_F-Score_1500 completed in 1558.2s\n",
      "   CV: 0.5884 Â± 0.0317\n",
      "   Train: 0.9400\n",
      "   Val: 0.6267 (gap: 0.3133)\n",
      "\n",
      "ğŸ“Š FINAL COMPARISON:\n",
      "   Original (5000 features): Your baseline\n",
      "   Selected (1500 features): 0.6267\n",
      "   Feature reduction: 70.0%\n",
      "   Training speedup: ~3.3x expected\n",
      "ğŸ’¾ Model saved to: ../models/trained/RandomForest/high_performance_with_feature_selection_F-Score_1500.pkl\n",
      "ğŸ’¾ Feature-selected model saved!\n"
     ]
    }
   ],
   "source": [
    "# Now optimize the best one\n",
    "print(f\"\\nğŸš€ Optimizing best feature selection...\")\n",
    "\n",
    "anti_overfitting_space = {\n",
    "    'n_estimators': (200, 500),           # More trees for stability\n",
    "    'max_depth': (3, 6),                  # Shallower trees\n",
    "    'min_samples_split': (20, 100),       # Larger splits\n",
    "    'min_samples_leaf': (15, 50),         # Larger leaves\n",
    "    'max_features': ['sqrt', 'log2', 0.3], # Fewer features per tree\n",
    "    'max_samples': (0.6, 0.9),           # Subsample training data\n",
    "}\n",
    "\n",
    "\n",
    "high_performance_space = {\n",
    "    'n_estimators': (300, 800),           # Many trees\n",
    "    'max_depth': (8, 15),                 # Deeper trees\n",
    "    'min_samples_split': (2, 15),         # Allow smaller splits\n",
    "    'min_samples_leaf': (1, 8),           # Allow smaller leaves\n",
    "    'max_features': [0.4, 0.6, 0.8, 'sqrt'], # More features\n",
    "}\n",
    "\n",
    "diversity_space = {\n",
    "    'n_estimators': (200, 400),\n",
    "    'max_depth': (4, 10),\n",
    "    'min_samples_split': (5, 50),         # Wide range\n",
    "    'min_samples_leaf': (2, 30),          # Wide range\n",
    "    'max_features': [0.2, 0.4, 'sqrt', 'log2'], # Include low feature counts\n",
    "    'max_samples': (0.5, 0.9),           # Vary subsampling\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "}\n",
    "\n",
    "balanced_space = {\n",
    "    'n_estimators': (100, 300),\n",
    "    'max_depth': (5, 12),\n",
    "    'min_samples_split': (10, 40),\n",
    "    'min_samples_leaf': (5, 25),\n",
    "    'max_features': ['sqrt', 'log2', 0.4, 0.6],\n",
    "    'criterion': ['gini', 'entropy'],      # Try both splitting criteria\n",
    "}\n",
    "\n",
    "rf_name = f\"high_performance_with_feature_selection_{best['name']}\"\n",
    "\n",
    "optimized_rf = RandomForestModel(\n",
    "    n_estimators=100,\n",
    "    max_depth=8,\n",
    "    min_samples_leaf=12,\n",
    "    random_state=42\n",
    ")\n",
    "optimized_rf.name = f\"Optimized_{best['name']}\"\n",
    "\n",
    "final_results = trainer.train_model(\n",
    "    model=optimized_rf,\n",
    "    X_train=best['X_train_sel'],\n",
    "    y_train=y_train,\n",
    "    X_val=best['X_val_sel'],\n",
    "    y_val=y_val,\n",
    "    param_space=high_performance_space,\n",
    "    optimize=True\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š FINAL COMPARISON:\")\n",
    "print(f\"   Original (5000 features): Your baseline\")\n",
    "print(f\"   Selected ({best['n_features']} features): {final_results['val_accuracy']:.4f}\")\n",
    "print(f\"   Feature reduction: {(1-best['n_features']/5000)*100:.1f}%\")\n",
    "print(f\"   Training speedup: ~{5000/best['n_features']:.1f}x expected\")\n",
    "\n",
    "# Save if better\n",
    "trainer.save_model(optimized_rf, f\"../models/trained/RandomForest/{rf_name}.pkl\")\n",
    "print(f\"ğŸ’¾ Feature-selected model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8478e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "304",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
