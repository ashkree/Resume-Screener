{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be7be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc450bdf-6484-46c8-af39-b78696ec5106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.TextPreprocessor import TextPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06325341",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3492953-3cc4-4392-b9e2-ff5bdbc0495c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and split:\n",
      "Training: 4368 samples\n",
      "Validation: 1873 samples\n",
      "Test: 1759 samples\n",
      "Label distribution in training:\n",
      "label\n",
      "No Fit           2200\n",
      "Potential Fit    1089\n",
      "Good Fit         1079\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from hugging face \n",
    "# cnamuangtoun/resume-job-description-fit\n",
    "\n",
    "ds = load_dataset(\"cnamuangtoun/resume-job-description-fit\")\n",
    "train_df = ds['train'].to_pandas()\n",
    "test_df = ds['test'].to_pandas()\n",
    "\n",
    "# Create train/validation split\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.30,\n",
    "                                   stratify=train_df[\"label\"], random_state=42)\n",
    "\n",
    "# Create label mapping\n",
    "label_to_id = {\"Good Fit\": 0, \"No Fit\": 1, \"Potential Fit\": 2}\n",
    "\n",
    "train_df[\"labels\"] = train_df[\"label\"].map(label_to_id)\n",
    "val_df[\"labels\"] = val_df[\"label\"].map(label_to_id)\n",
    "test_df[\"labels\"] = test_df[\"label\"].map(label_to_id)\n",
    "\n",
    "print(f\"Data loaded and split:\")\n",
    "print(f\"Training: {len(train_df)} samples\")\n",
    "print(f\"Validation: {len(val_df)} samples\")\n",
    "print(f\"Test: {len(test_df)} samples\")\n",
    "print(f\"Label distribution in training:\")\n",
    "print(train_df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9e913f7-7f0d-4f22-ac53-610d9f2b0d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying text preprocessing...\n",
      "Processing resume texts with comprehensive cleaning...\n",
      "Processing job description texts with comprehensive cleaning...\n",
      "Processing resume texts with comprehensive cleaning...\n",
      "Processing job description texts with comprehensive cleaning...\n",
      "Processing resume texts with comprehensive cleaning...\n",
      "Processing job description texts with comprehensive cleaning...\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying text preprocessing...\")\n",
    "\n",
    "\"\"\"\n",
    "Fixes common formatting issues like missing spaces, malformed sections,\n",
    "and inconsistent punctuation using regex patterns. \n",
    "\"\"\"\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Clean all datasets\n",
    "train_df = preprocessor.process_dataset(train_df)\n",
    "val_df = preprocessor.process_dataset(val_df)\n",
    "test_df = preprocessor.process_dataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "643887fb-fff8-4ecb-97d0-fa5aa2927c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining resume and job description texts...\n",
      "Combined texts created:\n",
      "Training: 4368 texts\n",
      "Validation: 1873 texts\n",
      "\n",
      "Example combined text (first 300 chars):\n",
      "Professional Summary Obtain a position in a professional organization where I can apply my skills and loyalty in exchange for career guidance, training and opportunity for advancement. Core Qualifications Microsoft Office (Word, Excel, Power Point, Access). FCR: Online Application for financial transactions. Experience Project Accountant,07/2012 to 12/2013 Jacobs Engineering Group Inc.-Tucker,, Afghanistan The project funded by USACE- United State Army Corps of Engineers Performed weekly Cash Counts and monthly Bank account reconciliations and reports back to the MTN / DC home office project accountant. Entered all transactions into the WEBFCR and uploaded backup to the WEBFCR on a daily basis Prepared cash flow projects for upcoming months (Cash forecast) and submitted the budget request every month. Uploaded all vendor/ contractor invoices to IMS and Ensured all payments are made in a timely manner to vendors and employees. Reviewed a limited variety of accounting documents and / or transactions to ensure proper supporting documentation has been submitted. Subcontract Accountant,02/2011 to 06/2012 Allied Universal-Lutz,, Afghanistan The program funded by USAID- United State Agency for International Development Reviewed all supplier / subcontractor invoices, bill and requests for payment transfer from LBG - B & V office to be reviewed and approved by Contract Manager, Task Order Manager and Chief of Party. Kept track of all sub - contracts documents, all payments confirmations sent from Head Quarter (DC office) and reviewed vouchers for wire transfer from Headquarter DC. Prepared all vouchers (disbursement, Cash, Bank and Advance journal vouchers) for expenditures and ensured that expenses are reasonable, allowable and allocable to the project, and coded all payments by account type using the GL Accounts. Preparation of weekly financial reports with Backups and send to HQ. Responsible to disbursement of all B & V Cash Payments and Petty Cash, Cash Book, Bank book and other B & V financial Activities. Administrative & Finance Coordinator,01/1 to 11/2010 Education Development Center, Inc-City,, Afghanistan The project funded by USAID- United State Agency for International Development Organized and preparing technical, administrative and financial files. Facilitated the lodging arrangements for any visitor, book flights for local and international staff traveling to the field and local transportation for international visitors. Maintained record keeping system of all office administrative and financial files. Handled the sending, receiving and distributing of all correspondence between the Kabul and Washington DC; served as the main point of contact for EDC / Washington. Assisted in purchasing materials for training workshops and other activities. Prepared payments for the procurement of materials, equipment, furniture and stationary for the project on timely basis. Prepared monthly and regular reports of Expense Vouchers, Advance Vouchers, and Bank Vouchers. Administrative Officer,09/2006 to 05/2010-,, CETENA GROUP - Kabul, Afghanistan Developed a filing system, established, and maintained a standard system to ensure Files tracking of IED (Improvised Explosive Devices) Project. Organized data and information, prepared and maintained records, documents and control plans for the monitoring of IED (Improvised Explosive Devices) project. Facilitated new/ extend visa for the International Staff, follow - up with the flight booking, confirmation and cancellation and ensured the work permits and visa for international staffs were up - to - date. Performed other Administration duties. Education:, Expected in 1 2016 to Virginia International University-, GPA: Master‚Äôs in Business Administration (International Business) Expected spring Bachelor: Business Administration Finance, Expected in 1 2012 to Kardan University-, GPA: Business Administration Finance Diploma: Business Administration, Expected in 1 2006 to Capital Institute of Information Technology-, GPA: Business Administration:, Expected in 1 2006 to Khurasan High School-, GPA: Skillsaccounting, accountant, administrative, Army, Agency, backup, book, budget, Business Administration, cash flow, contracts, DC, documentation, filing, financial, GL, home office, IMS, International Business, materials, Access, Excel, Microsoft Office, office, Power Point, Word, procurement, purchasing, receiving, record keeping, transportation, type, workshops [SEP] Job Purpose: The Senior Accountant is responsible for bookkeeping including maintaining company financial records and monthly billing. This position is also responsible for accounts receivable and accounts payable, payroll, payroll taxes, quarterly payroll tax reports, invoicing, personnel payroll records and business reports. Qualifications and Requirements: Bachelors degree in Accounting or Finance or related field required. Minimum five (5) to Seven (7) years of experience working in a combination of booking, payroll, APAR, senior level position required. Advanced knowledge of Generally Accepted Accounting Principles (GAAP) and accounting software required\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Combine resume and job description texts\n",
    "def combine_texts(resume_text, job_desc_text):\n",
    "    \"\"\"Combine resume and job description for feature extraction\"\"\"\n",
    "    return resume_text + \" [SEP] \" + job_desc_text\n",
    "\n",
    "print(\"Combining resume and job description texts...\")\n",
    "\n",
    "# Create combined texts for training and validation\n",
    "train_combined = [combine_texts(row['resume_text'], row['job_description_text']) \n",
    "                  for _, row in train_df.iterrows()]\n",
    "val_combined = [combine_texts(row['resume_text'], row['job_description_text']) \n",
    "                for _, row in val_df.iterrows()]\n",
    "\n",
    "print(f\"Combined texts created:\")\n",
    "print(f\"Training: {len(train_combined)} texts\")\n",
    "print(f\"Validation: {len(val_combined)} texts\")\n",
    "\n",
    "print(f\"\\nExample combined text (first 300 chars):\")\n",
    "print(train_combined[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d16d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_subset(df, target_size, label_col='labels', random_state=42):\n",
    "    \"\"\"\n",
    "    Create a balanced subset of specified size\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to subset\n",
    "        target_size: Total target size for subset\n",
    "        label_col: Column containing labels\n",
    "        random_state: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        Balanced subset DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get class distribution\n",
    "    class_counts = Counter(df[label_col])\n",
    "    n_classes = len(class_counts)\n",
    "    \n",
    "    print(f\"Original size: {len(df)}\")\n",
    "    print(f\"Original distribution: {dict(class_counts)}\")\n",
    "    \n",
    "    # Calculate samples per class for balanced subset\n",
    "    samples_per_class = target_size // n_classes\n",
    "    \n",
    "    print(f\"Target subset size: {target_size}\")\n",
    "    print(f\"Samples per class: {samples_per_class}\")\n",
    "    \n",
    "    # Check if we have enough samples in each class\n",
    "    min_available = min(class_counts.values())\n",
    "    if samples_per_class > min_available:\n",
    "        samples_per_class = min_available\n",
    "        actual_size = samples_per_class * n_classes\n",
    "        print(f\"‚ö†Ô∏è  Adjusted to {samples_per_class} per class (total: {actual_size})\")\n",
    "    \n",
    "    # Sample from each class\n",
    "    subset_dfs = []\n",
    "    for label in sorted(class_counts.keys()):\n",
    "        class_df = df[df[label_col] == label]\n",
    "        sampled_df = class_df.sample(n=samples_per_class, random_state=random_state)\n",
    "        subset_dfs.append(sampled_df)\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    balanced_subset = pd.concat(subset_dfs, ignore_index=True)\n",
    "    balanced_subset = balanced_subset.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    # Verify balance\n",
    "    final_counts = Counter(balanced_subset[label_col])\n",
    "    print(f\"Final distribution: {dict(final_counts)}\")\n",
    "    print(f\"Final size: {len(balanced_subset)}\")\n",
    "    \n",
    "    return balanced_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d88d6a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating balanced training subset...\n",
      "Original size: 4368\n",
      "Original distribution: {1: 2200, 2: 1089, 0: 1079}\n",
      "Target subset size: 3000\n",
      "Samples per class: 1000\n",
      "Final distribution: {1: 1000, 0: 1000, 2: 1000}\n",
      "Final size: 3000\n",
      "\n",
      "Creating balanced validation subset...\n",
      "Original size: 1873\n",
      "Original distribution: {0: 463, 1: 943, 2: 467}\n",
      "Target subset size: 1200\n",
      "Samples per class: 400\n",
      "Final distribution: {2: 400, 0: 400, 1: 400}\n",
      "Final size: 1200\n",
      "\n",
      "Creating balanced test subset...\n",
      "Original size: 1759\n",
      "Original distribution: {1: 857, 2: 444, 0: 458}\n",
      "Target subset size: 300\n",
      "Samples per class: 100\n",
      "Final distribution: {2: 100, 1: 100, 0: 100}\n",
      "Final size: 300\n"
     ]
    }
   ],
   "source": [
    "# Create balanced subsets\n",
    "print(\"Creating balanced training subset...\")\n",
    "train_subset = create_balanced_subset(train_df, target_size=3000, random_state=42)\n",
    "\n",
    "print(f\"\\nCreating balanced validation subset...\")\n",
    "val_subset = create_balanced_subset(val_df, target_size=1200, random_state=42)\n",
    "\n",
    "print(f\"\\nCreating balanced test subset...\")  \n",
    "test_subset = create_balanced_subset(test_df, target_size=300, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ea4f583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Adding feature extractors...\n",
      "‚úÖ Added TF-IDF to pipeline\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<feature_extractors.FeatureExtractionPipeline.FeatureExtractionPipeline at 0x7f95480f8f50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set feature extractors - allows for easy addition of other extraction techniques\n",
    "\n",
    "from feature_extractors.TfidfFeatureExtractor import TfidfFeatureExtractor\n",
    "from feature_extractors.FeatureExtractionPipeline import FeatureExtractionPipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = FeatureExtractionPipeline()\n",
    "\n",
    "# Add extractors\n",
    "print(\"üîß Adding feature extractors...\")\n",
    "\n",
    "# TF-IDF extractor\n",
    "tfidf_extractor = TfidfFeatureExtractor(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=3,\n",
    "    max_df=0.85\n",
    ")\n",
    "\n",
    "pipeline.add_extractor(tfidf_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2ef5129-57f2-4024-9e9a-b4b3a79c2635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE EXTRACTION PIPELINE\n",
      "============================================================\n",
      "üîß Preparing training texts...\n",
      "‚úÖ Prepared 3000 training texts\n",
      "\n",
      "üöÄ Fitting 1 extractors...\n",
      "Fitting TF-IDF on 3000 documents...\n",
      "‚úÖ TF-IDF fitted in 1.91 seconds\n",
      "   ‚úì TF-IDF fitted successfully\n",
      "\n",
      "--- TRANSFORMING TRAINING DATA ---\n",
      "üîÑ Transforming 3000 texts with 1 extractors...\n",
      "   ‚úì TF-IDF: (3000, 5000), density=0.0964, 1.34s\n",
      "\n",
      "--- TRANSFORMING VALIDATION DATA ---\n",
      "üîÑ Transforming 1200 texts with 1 extractors...\n",
      "   ‚úì TF-IDF: (1200, 5000), density=0.0960, 0.53s\n",
      "\n",
      "‚úÖ Feature extraction completed for 1 extractors\n",
      "üîÑ Transforming 300 texts with 1 extractors...\n",
      "   ‚úì TF-IDF: (300, 5000), density=0.0969, 0.14s\n",
      "‚úÖ Feature extraction completed!\n",
      "üìä Results summary:\n",
      "   TF-IDF: Train(3000, 5000), Val(1200, 5000), Density:0.0964\n"
     ]
    }
   ],
   "source": [
    "# Extract features using the pipeline\n",
    "try:\n",
    "    results = pipeline.extract_features(train_subset, val_subset)\n",
    "    test_results = pipeline.transform_all(test_subset)\n",
    "    \n",
    "    print(f\"‚úÖ Feature extraction completed!\")\n",
    "    print(f\"üìä Results summary:\")\n",
    "    for name, result in results.items():\n",
    "        train_shape = result['train']['shape']\n",
    "        val_shape = result['val']['shape']\n",
    "        density = result['train']['density']\n",
    "        print(f\"   {name}: Train{train_shape}, Val{val_shape}, Density:{density:.4f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Pipeline extraction failed: {e}\")\n",
    "    print(\"Let's debug step by step...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17993847-3a95-4b01-9cfc-cf69e4d8eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features directly (you only have TF-IDF anyway)\n",
    "X_train = results['TF-IDF']['train']['features']\n",
    "X_val = results['TF-IDF']['val']['features']\n",
    "X_test = test_results['TF-IDF']['features']\n",
    "\n",
    "# Get labels\n",
    "y_train = train_subset['labels'].values\n",
    "y_val = val_subset['labels'].values\n",
    "y_test = test_subset['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cdb3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"../data/processed/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Export features\n",
    "with open(f\"{output_dir}/X_train.pkl\", 'wb') as f:\n",
    "    pickle.dump(X_train, f)\n",
    "\n",
    "with open(f\"{output_dir}/X_val.pkl\", 'wb') as f:\n",
    "    pickle.dump(X_val, f)\n",
    "\n",
    "with open(f\"{output_dir}/X_test.pkl\", 'wb') as f:\n",
    "    pickle.dump(X_test, f)\n",
    "\n",
    "# Export labels\n",
    "with open(f\"{output_dir}/y_train.pkl\", 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "\n",
    "with open(f\"{output_dir}/y_val.pkl\", 'wb') as f:\n",
    "    pickle.dump(y_val, f)\n",
    "\n",
    "with open(f\"{output_dir}/y_test.pkl\", 'wb') as f:\n",
    "    pickle.dump(y_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a2c36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "304",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
