{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8169ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48471841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import multiprocessing\n",
    "warnings.filterwarnings(\"ignore\", category=ResourceWarning)\n",
    "\n",
    "# Also suppress multiprocessing warnings\n",
    "import sys\n",
    "import os\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore::ResourceWarning'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70956af3",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43c1f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def load_datasets(data_root: str | Path = \"data\",\n",
    "                  tasks: tuple[str, ...] = (\"binary\", \"multiclass\"),\n",
    "                  splits: tuple[str, ...] = (\"train\", \"val\", \"test\")) -> dict:\n",
    "\n",
    "    data_root = Path(data_root)\n",
    "    datasets  = {}\n",
    "\n",
    "    for task in tasks:\n",
    "        task_dir     = data_root / task\n",
    "        task_dict    = {}\n",
    "\n",
    "        for split in splits:\n",
    "            split_dict = {}\n",
    "            for kind in (\"X\", \"y\"):\n",
    "                file_path = task_dir / f\"{kind}_{split}.pkl\"\n",
    "                split_dict[kind] = pd.read_pickle(file_path)\n",
    "            task_dict[split] = split_dict\n",
    "\n",
    "        datasets[task] = task_dict\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8999d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Literal\n",
    "import pandas as pd\n",
    "\n",
    "def load_split(\n",
    "    preprocessing_type: Literal[\"cleaned_only\", \"full_process\"],\n",
    "    sampling_method: Literal[\"undersampled\", \"oversampled\"],\n",
    "    classification_type: Literal[\"binary\", \"multiclass\"]\n",
    ") -> Tuple[\n",
    "    Tuple[pd.DataFrame, pd.Series],  # train: (X_train, y_train)\n",
    "    Tuple[pd.DataFrame, pd.Series],  # val: (X_val, y_val)\n",
    "    Tuple[pd.DataFrame, pd.Series]   # test: (X_test, y_test)\n",
    "]:\n",
    "    \"\"\"\n",
    "    Load different types of splits from the data\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_type: must be \"cleaned_only\" or \"full_process\"\n",
    "        sampling_method: must be \"undersampled\" or \"oversampled\"\n",
    "        classification_type: must be \"binary\" or \"multiclass\"\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train, val, test) splits, where each split is (X, y)\n",
    "        - train: (X_train, y_train)\n",
    "        - val: (X_val, y_val)  \n",
    "        - test: (X_test, y_test)\n",
    "    \"\"\"\n",
    "    dataset = load_datasets(\n",
    "        f\"../data/{preprocessing_type}/{sampling_method}\")[classification_type]\n",
    "    split_names = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "    return tuple([(lambda split: (dataset[split][\"X\"], dataset[split][\"y\"]))(split) for split in split_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5ac5e9",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91a67f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(X):\n",
    "    X = X.copy()\n",
    "\n",
    "    combined = X[\"resume_text\"].astype(\n",
    "        str) + \" [SEP] \" + X[\"job_description_text\"].astype(str)\n",
    "\n",
    "    return combined.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9ebac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = load_split(preprocessing_type=\"cleaned_only\", sampling_method=\"undersampled\", classification_type=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cec0d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b0fa8",
   "metadata": {},
   "source": [
    "## Experiment 4: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34000bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ExperimentManager, Experiment\n",
    "feature_engineering_manager = ExperimentManager(f\"../runs/ensemble/feature_engineering\", [\"Fit\", \"Not Fit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "065858e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Experiment: FE 1 cosine sim EBM stack ===\n",
      "\n",
      "============================================================\n",
      "ðŸŽ¯ TEST SET EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š OVERVIEW\n",
      "   Test Samples: 1,714\n",
      "   Classes: 2\n",
      "   Overall Accuracy: 0.6832\n",
      "\n",
      "ðŸŽ¯ MAIN PERFORMANCE METRICS\n",
      "   Macro F1:     0.6821\n",
      "   Micro F1:     0.6832\n",
      "   Weighted F1:  0.6821\n",
      "\n",
      "ðŸ“ˆ PRECISION/RECALL SUMMARY\n",
      "   Macro    - P: 0.6857  R: 0.6832\n",
      "   Micro    - P: 0.6832  R: 0.6832\n",
      "   Weighted - P: 0.6857  R: 0.6832\n",
      "\n",
      "ðŸ“‹ DETAILED CLASSIFICATION REPORT\n",
      "   Class             Precision     Recall   F1-Score    Support\n",
      "   ---------------- ---------- ---------- ---------- ----------\n",
      "   Fit                  0.6642     0.7410     0.7005        857\n",
      "   Not Fit              0.7071     0.6254     0.6638        857\n",
      "   ---------------- ---------- ---------- ---------- ----------\n",
      "   macro avg            0.6857     0.6832     0.6821       1714\n",
      "   weighted avg         0.6857     0.6832     0.6821       1714\n",
      "\n",
      "ðŸ”¢ CONFUSION MATRIX\n",
      "   Rows: True Labels, Columns: Predicted Labels\n",
      "   Predicted â†’\n",
      "   True â†“        Fit  Not Fit \n",
      "   Fit           635      222 \n",
      "   Not Fit       321      536 \n",
      "\n",
      "============================================================\n",
      "ðŸ’¾ Finalizing experiment...\n",
      "\n",
      "âœ… Experiment 'FE 1 cosine sim EBM stack' completed and logged.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<utils.ExperimentManger.Experiment at 0x7fccde5ebf20>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import FeatureUnion, FunctionTransformer, Pipeline\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "def compute_cosine_similarity(X):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between resume_text and job_description_text\n",
    "    for each row in a DataFrame or compatible input.\n",
    "    Returns a 2D NumPy array of shape (n_samples, 1).\n",
    "    \"\"\"\n",
    "\n",
    "    # Defensive: ensure X is a DataFrame with expected columns\n",
    "    if isinstance(X, np.ndarray):\n",
    "        # If it's already an ndarray, we must know column order\n",
    "        X = pd.DataFrame(X, columns=[\"resume_text\", \"job_description_text\"])\n",
    "    elif not isinstance(X, pd.DataFrame):\n",
    "        raise ValueError(\"Input X must be a DataFrame or 2D ndarray.\")\n",
    "\n",
    "    if \"resume_text\" not in X.columns or \"job_description_text\" not in X.columns:\n",
    "        raise ValueError(\"Expected columns 'resume_text' and 'job_description_text' not found.\")\n",
    "\n",
    "    # Flatten all text for vectorizer fit\n",
    "    all_texts = X[\"resume_text\"].astype(str).tolist() + X[\"job_description_text\"].astype(str).tolist()\n",
    "    \n",
    "    # Fit vectorizer\n",
    "    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "    vectorizer.fit(all_texts)\n",
    "\n",
    "    # Compute cosine similarity for each row\n",
    "    cosine_scores = []\n",
    "    for idx, row in X.iterrows():\n",
    "        resume_text = str(row['resume_text'])\n",
    "        job_text = str(row['job_description_text'])\n",
    "\n",
    "        tfidf_matrix = vectorizer.transform([resume_text, job_text])\n",
    "        cos_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "        cosine_scores.append(cos_sim)\n",
    "\n",
    "    return np.array(cosine_scores).reshape(-1, 1)\n",
    "\n",
    "def pipeline_factory(params):\n",
    "    # Base classifiers with some tuning\n",
    "    clf_lr = LogisticRegression(random_state=SEED)\n",
    "    \n",
    "    clf_rf = RandomForestClassifier(random_state=SEED)\n",
    "    \n",
    "    clf_nb = BernoulliNB()\n",
    "    \n",
    "    # Stacking classifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', clf_lr),\n",
    "            ('nb', clf_nb),\n",
    "            ('rf', clf_rf)\n",
    "        ],\n",
    "        final_estimator=ExplainableBoostingClassifier(random_state=SEED),\n",
    "        cv=20,\n",
    "        n_jobs=1  # Use all available cores\n",
    "    )\n",
    "    \n",
    "    return Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            # TF-IDF features\n",
    "            ('tfidf_features', Pipeline([\n",
    "                (\"join\", FunctionTransformer(combine_text, validate=False)),\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('selector', SelectKBest(chi2, k=100))\n",
    "            ])),\n",
    "            \n",
    "            # Cosine similarity feature with scaling\n",
    "            ('cosine_sim', Pipeline([\n",
    "                ('extract', FunctionTransformer(compute_cosine_similarity, validate=False))\n",
    "            ]))\n",
    "        ])),\n",
    "        ('clf', stacking_clf)\n",
    "    ])\n",
    "\n",
    "experiment = Experiment(\n",
    "name=f\"FE 1 cosine sim EBM stack\",\n",
    "description=f\"Stack ensemble with EBM and cosine sim feature engineering\",\n",
    "pipeline_factory=pipeline_factory\n",
    ")\n",
    "\n",
    "feature_engineering_manager.run_experiment(experiment, splits=splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47f359cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from skillNer.general_params import SKILL_DB\n",
    "from skillNer.skill_extractor_class import SkillExtractor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "class SkillNERCountTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Return two columns: [resume_skill_cnt, jd_skill_cnt].\"\"\"\n",
    "    \n",
    "    def __init__(self, model=\"en_core_web_sm\"):\n",
    "        self.model = model\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state.pop(\"nlp\", None)\n",
    "        state.pop(\"skill_extractor\", None)\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        self.__dict__.update(state)\n",
    "        self._lazy_init()\n",
    "    \n",
    "    def _lazy_init(self):\n",
    "        if not hasattr(self, \"skill_extractor\"):\n",
    "            import spacy\n",
    "            from spacy.matcher import PhraseMatcher\n",
    "            from skillNer.general_params import SKILL_DB\n",
    "            from skillNer.skill_extractor_class import SkillExtractor\n",
    "            \n",
    "            self.nlp = spacy.load(self.model)\n",
    "            self.skill_extractor = SkillExtractor(\n",
    "                self.nlp, SKILL_DB, PhraseMatcher\n",
    "            )\n",
    "    \n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Clean and preprocess text to avoid SkillNER issues\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        \n",
    "        # Remove or replace problematic characters\n",
    "        text = re.sub(r'[^\\w\\s\\-\\.\\,\\;\\:\\!\\?\\(\\)\\[\\]\\/\\@\\#\\$\\%\\&\\*\\+\\=\\<\\>\\'\\\"]', ' ', text)\n",
    "        \n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Ensure text is not empty\n",
    "        if not text or text.isspace():\n",
    "            return \"No content available\"\n",
    "        \n",
    "        # Limit text length to avoid memory issues\n",
    "        if len(text) > 10000:\n",
    "            text = text[:10000]\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _safe_skill_extraction(self, text):\n",
    "        \"\"\"Safely extract skills with error handling\"\"\"\n",
    "        try:\n",
    "            cleaned_text = self._clean_text(text)\n",
    "            result = self.skill_extractor.annotate(cleaned_text)\n",
    "            \n",
    "            # Extract counts safely\n",
    "            full_matches = len(result.get(\"results\", {}).get(\"full_matches\", []))\n",
    "            ngram_scored = len(result.get(\"results\", {}).get(\"ngram_scored\", []))\n",
    "            \n",
    "            return full_matches + ngram_scored\n",
    "            \n",
    "        except (IndexError, KeyError, AttributeError, ValueError) as e:\n",
    "            print(f\"Warning: SkillNER extraction failed: {e}\")\n",
    "            return 0\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error in skill extraction: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X, columns=[\"resume_text\", \"job_description_text\"])\n",
    "        \n",
    "        self._lazy_init()\n",
    "        \n",
    "        res_cnt, jd_cnt = [], []\n",
    "        \n",
    "        for _, row in X.iterrows():\n",
    "            # Safe extraction with error handling\n",
    "            resume_skills = self._safe_skill_extraction(row[\"resume_text\"])\n",
    "            jd_skills = self._safe_skill_extraction(row[\"job_description_text\"])\n",
    "            \n",
    "            res_cnt.append(resume_skills)\n",
    "            jd_cnt.append(jd_skills)\n",
    "        \n",
    "        # Return 2-column CSR matrix\n",
    "        dense = np.c_[res_cnt, jd_cnt]\n",
    "        return sparse.csr_matrix(dense, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875a1e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Experiment: FE 2 cosine sim + skillNER EBM stack ===\n",
      "loading full_matcher ...\n",
      "loading abv_matcher ...\n",
      "loading full_uni_matcher ...\n",
      "loading low_form_matcher ...\n",
      "loading token_matcher ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maveron/.conda/envs/304/lib/python3.12/site-packages/skillNer/utils.py:99: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  vec_similarity = token1.similarity(token2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: SkillNER extraction failed: list index out of range\n"
     ]
    }
   ],
   "source": [
    "def pipeline_factory(params):\n",
    "    # Base classifiers\n",
    "    clf_lr = LogisticRegression(random_state=SEED)\n",
    "    \n",
    "    clf_rf = RandomForestClassifier(random_state=SEED,)\n",
    "    \n",
    "    clf_nb = BernoulliNB()\n",
    "    \n",
    "    # Stacking classifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', clf_lr),\n",
    "            ('nb', clf_nb),\n",
    "            ('rf', clf_rf)\n",
    "        ],\n",
    "        final_estimator=ExplainableBoostingClassifier(random_state=SEED,),\n",
    "        cv=20,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    \n",
    "    return Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            # TF-IDF features\n",
    "            ('tfidf_features', Pipeline([\n",
    "                (\"join\", FunctionTransformer(combine_text, validate=False)),\n",
    "                ('tfidf', TfidfVectorizer(max_features=3000)),\n",
    "                ('selector', SelectKBest(chi2, k=100))\n",
    "            ])),\n",
    "            \n",
    "            # Cosine similarity feature (unscaled - as you found works better)\n",
    "            ('cosine_sim', FunctionTransformer(compute_cosine_similarity, validate=False)),\n",
    "            \n",
    "            # SkillNER features (skill counts)\n",
    "            ('skillner_features', SkillNERCountTransformer())\n",
    "        ])),\n",
    "        ('clf', stacking_clf)\n",
    "    ])\n",
    "\n",
    "# Updated experiment\n",
    "experiment = Experiment(\n",
    "    name=f\"FE 2 cosine sim + skillNER EBM stack\",\n",
    "    description=f\"Stack ensemble with EBM, cosine sim, and SkillNER feature engineering\",\n",
    "    pipeline_factory=pipeline_factory\n",
    ")\n",
    "\n",
    "feature_engineering_manager.run_experiment(experiment, splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa11575d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "304",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
