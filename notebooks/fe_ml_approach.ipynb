{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc450bdf-6484-46c8-af39-b78696ec5106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import all required libraries\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import TextPreprocessor as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89981905-a83b-4cd0-9841-2b15a9f20907",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3492953-3cc4-4392-b9e2-ff5bdbc0495c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and split:\n",
      "Training: 5304 samples\n",
      "Validation: 937 samples\n",
      "Test: 1759 samples\n",
      "Label distribution in training:\n",
      "label\n",
      "No Fit           2671\n",
      "Potential Fit    1322\n",
      "Good Fit         1311\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"cnamuangtoun/resume-job-description-fit\")\n",
    "train_df = ds['train'].to_pandas()\n",
    "test_df = ds['test'].to_pandas()\n",
    "\n",
    "# Create train/validation split\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.15,\n",
    "                                   stratify=train_df[\"label\"], random_state=42)\n",
    "\n",
    "# Create label mapping\n",
    "label_to_id = {\"Good Fit\": 0, \"No Fit\": 1, \"Potential Fit\": 2}\n",
    "id_to_label = {0: \"Good Fit\", 1: \"No Fit\", 2: \"Potential Fit\"}\n",
    "\n",
    "train_df[\"labels\"] = train_df[\"label\"].map(label_to_id)\n",
    "val_df[\"labels\"] = val_df[\"label\"].map(label_to_id)\n",
    "test_df[\"labels\"] = test_df[\"label\"].map(label_to_id)\n",
    "\n",
    "print(f\"Data loaded and split:\")\n",
    "print(f\"Training: {len(train_df)} samples\")\n",
    "print(f\"Validation: {len(val_df)} samples\")\n",
    "print(f\"Test: {len(test_df)} samples\")\n",
    "print(f\"Label distribution in training:\")\n",
    "print(train_df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbce00b-8056-47db-bcf6-f64e867fc2a9",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e913f7-7f0d-4f22-ac53-610d9f2b0d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying text preprocessing...\n",
      "Processing resume texts with comprehensive cleaning...\n",
      "Processing job description texts with comprehensive cleaning...\n",
      "Processing resume texts with comprehensive cleaning...\n",
      "Processing job description texts with comprehensive cleaning...\n",
      "Processing resume texts with comprehensive cleaning...\n",
      "Processing job description texts with comprehensive cleaning...\n",
      "Text preprocessing completed!\n",
      "\n",
      "Example of cleaned text:\n",
      "Resume (first 200 chars): Professional Summary With the attitude of learning I am looking for an internship from summer 2017 to gain as much as knowledge as I can and contribute to the organizations success. Core Qualification\n",
      "Job Description (first 200 chars): Immediate need 3-Month Contract to Hire- no C 2 C consultants Must be eligible for hire without sponsorship 100% remote working EST hours Must Haves: 5 years of experience AWS, Java with Spring boot, \n"
     ]
    }
   ],
   "source": [
    "print(\"Applying text preprocessing...\")\n",
    "preprocessor = tp.TextPreprocessor()\n",
    "\n",
    "# Clean all datasets\n",
    "train_df = preprocessor.process_dataset(train_df)\n",
    "val_df = preprocessor.process_dataset(val_df)\n",
    "test_df = preprocessor.process_dataset(test_df)\n",
    "\n",
    "print(\"Text preprocessing completed!\")\n",
    "\n",
    "# Show a before/after example\n",
    "print(\"\\nExample of cleaned text:\")\n",
    "print(\"Resume (first 200 chars):\", train_df.iloc[0]['resume_text'][:200])\n",
    "print(\"Job Description (first 200 chars):\", train_df.iloc[0]['job_description_text'][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da417bf-9a92-47f8-be2a-85881a67214c",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "643887fb-fff8-4ecb-97d0-fa5aa2927c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining resume and job description texts...\n",
      "Combined texts created:\n",
      "Training: 5304 texts\n",
      "Validation: 937 texts\n",
      "\n",
      "Example combined text (first 300 chars):\n",
      "Professional Summary With the attitude of learning I am looking for an internship from summer 2017 to gain as much as knowledge as I can and contribute to the organizations success. Core Qualifications C, C++, C#, SQL pl / sql Operating Systems: Windows, Linux, unix HDL / HVL: Verilog, System Verilog Scripting Language: Unix Shell Scripting, PERL, Python, TCLS oftware proficiency: Cadence (Layout, Virtuoso, Spectre), Synopsys (DC Compiler), Modelsim, Questasim, TFS for version controller, Microsoft sql server 2012, Visual studio 2008 Experience Software Engineer,12/2015-08/2016 Torch Technologies, Inc.-Corpus Christi, TX, India Analyze internal processes and recommend and implement procedural changes to improve operations in database. Analyze the requirement and make the necessary changes in the existing modules. Support and maintenance of the specific module. System Engineer,04/2013-12/2015 Apex Systems-Minnetonka, MN, India Worked in SDLC and Agile framework made me flexible in working as and when required Consistent performance and ownership of the task while maintaining of the module in Data ware house project. Interacting with clients (UK United Biscuits, Canada TD bank, ING bank Singapore) during analysis, development & implementation phase gave me a global exposure of interacting with the people across different regions of world. Understanding the requirement of the customers while providing customer support and testing of functional and technical queries, request, requirements and modifications. Conducted training sessions to train juniors for the same technology. Achievements Customer Satisfaction Index for the project increased from 82.7% to 97.8% in the duration of 6 months for the consistent performance and various suggestions and effective solutions being provided for the improvement of the system. Received an appreciation from VP of TD bank Mr. Barkla for a successful delivery of the compliance project within the Time lines. On the spot award\" for leading and managing event for the account (TCS). Safety Champion of Women safety initiative in the organization for 2015. I was leading a team of 10-15 people for a year in which I was interacting with various employees in the office, explaining them the significance of ergonomics, suggested exercises and also encouraging them to practice the same. Received All India Mahindra talent scholarship in the year of Aug-2006 for securing 85.73% JRD TATA scholarship in the year of Sept-2009 of for the accomplishment of 84.12% in the diploma. Education Masters: Electrical & Computer Engineering, Expected in Fall 2016-PORTLAND STATE UNIVERSITY-, Oregon GPA: Status - Electrical & Computer Engineering 4.0 Digital IC and Design (ECE 525) Ahsan *ASIC Design Modelling & Synthesis (ECE 581) Xiaoyu Song *Verilog Workshop (ECE 510 ) Roy Kravitz.:, Expected in--, GPA: Status - Microprocessor System Design (ECE 585) Mark Faust *Embedded Systems Design with FPGA s (ECE 544) Hammerstrom *Pre - silicon Functional Verification by Prof. Schubert:, Expected in - Mumbai University-, MHGPA: Status - Bachelor of Engineering: Electronics & Telecommunications, Expected in July 2012--, GPA: Status - Electronics & Telecommunications 67/100 Core Subjects: Digital logic design Digital communication Electronics devices and circuits I Analog digital IC design applications Electronics devices and circuits II Simulation of software workshop Microprocessor and micro - controllers I Microprocessor and micro - controllers IID iploma: Electronics & communications, Expected in July 2009-MAHARASHTRA STATE BOARD OF TECHNICAL EDUCATION - Mumbai, MHGPA: Status - Electronics & communications Grade: 84.12/100 Core Subjects: Electronic components and devices Programming in C Digital techniques and microprocessor Micro - controllers Digital communication Embedded systems Skills Agile, ASIC, C, C++, Cadence, controller, clients, Customer Satisfaction, customer support, database, DC, delivery, Electronics, Embedded Systems, ergonomics, Functional, Layout, Linux, logic, managing, Mark, Microprocessor, C#, office, 97, Windows, Operating Systems, pl / sql, PERL, processes, Programming, Python, requirement, Safety, SDLC, Scripting, Simulation, Microsoft sql server, SQL, System Design, TCL, unix, Unix Shell Scripting, Verilog, Visual studio [SEP] Immediate need 3-Month Contract to Hire- no C 2 C consultants Must be eligible for hire without sponsorship 100% remote working EST hours Must Haves: 5 years of experience AWS, Java with Spring boot, and Microservices. Must have back - end REST API experience. Experience working in an agile environment Overview:?? A mortgage client is seeking a Software Engineer 100% remote to join the team. This role is a contract - to - hire opportunity.?? Job Summary:?? The Software Engineer is a full - stack engineer with strong Node. js and functional programming (Scala & Ruby) skills. Our applications leverage Node. js, Bootstrap, JQ uery, Mondo DB, Elastic Search, Redis, React. js, and delightful interactive experiences on the web. Our applications run in the AWS cloud environment. We use Agile Methodologies to enable our engineering team to work closely with partners and with our design & product teams.? Responsibilities:?? Work as an individual contributor building software. Write maintainable and scalable backend and client web application code. Work with enterprise architect to build ESB layer and implement scalable web services layer (RESTJSONXML) to serve internal and external application clients. Work with designers and product managers to build web applications with responsive design. Work in an agile team environment where testing and continuous deployment are an integral part of the way you work. Bring clean code principles to your work and drive innovation through rapid prototyping and iterative development. Will be working with AWS RDS & Dynamo DB. Maintain regular and punctual attendance.? Required Skills and Experience:? Bachelors degree in Computer Science or equivalent is preferred, Capable of designing and coding highly efficient and scalable enterprise software and services. Fluent in Java, Backbone. js, Node. js, Bootstrap 3, Require. js, and D 3 or Highcharts. Experience with MV* application design, Java Script graphing, Modular Design (AMD, Common JS), and Responsive Design. Strong familiarity with functional programming such as Scala and Ruby. Extensive experience with SQL, database programming, and performance. Excellent understanding of web technologies and Internet architecture. Comfortable running your own *NIX server, web stack, debuggers, VM s, and database programs. Scripting language skills such as Perl, Python, and Bash are a strong plus. Experience in Hadoop, Apache Spark Big Data environment (Hive, Pig, Map Reduce) is an advantage. Strong verbalwritten communication and data presentation skills, including an ability to effectively communicate with both business and technical teams Strong experience with AWS environment, Git, and Continuous Integration (Jenkins).\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Combine resume and job description texts\n",
    "def combine_texts(resume_text, job_desc_text):\n",
    "    \"\"\"Combine resume and job description for feature extraction\"\"\"\n",
    "    return resume_text + \" [SEP] \" + job_desc_text\n",
    "\n",
    "print(\"Combining resume and job description texts...\")\n",
    "\n",
    "# Create combined texts for training and validation\n",
    "train_combined = [combine_texts(row['resume_text'], row['job_description_text']) \n",
    "                  for _, row in train_df.iterrows()]\n",
    "val_combined = [combine_texts(row['resume_text'], row['job_description_text']) \n",
    "                for _, row in val_df.iterrows()]\n",
    "\n",
    "print(f\"Combined texts created:\")\n",
    "print(f\"Training: {len(train_combined)} texts\")\n",
    "print(f\"Validation: {len(val_combined)} texts\")\n",
    "\n",
    "print(f\"\\nExample combined text (first 300 chars):\")\n",
    "print(train_combined[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d9bf6dd-fba0-40fa-98d6-24d207b66519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Adding feature extractors...\n",
      "‚úÖ Added TF-IDF to pipeline\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<feature_extractors.FeatureExtractionPipeline.FeatureExtractionPipeline at 0x7ff208442270>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from feature_extractors.TfidfFeatureExtractor import TfidfFeatureExtractor\n",
    "from feature_extractors.FeatureExtractionPipeline import FeatureExtractionPipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = FeatureExtractionPipeline()\n",
    "\n",
    "# Add extractors\n",
    "print(\"üîß Adding feature extractors...\")\n",
    "\n",
    "# TF-IDF extractor\n",
    "tfidf_extractor = TfidfFeatureExtractor(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=3,\n",
    "    max_df=0.85\n",
    ")\n",
    "pipeline.add_extractor(tfidf_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2ef5129-57f2-4024-9e9a-b4b3a79c2635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXTRACTING FEATURES ===\n",
      "üìä Using subset for testing:\n",
      "   Training: 1000 samples\n",
      "   Validation: 200 samples\n",
      "============================================================\n",
      "FEATURE EXTRACTION PIPELINE\n",
      "============================================================\n",
      "üîß Preparing training texts...\n",
      "‚úÖ Prepared 1000 training texts\n",
      "\n",
      "üöÄ Fitting 1 extractors...\n",
      "Fitting TF-IDF on 1000 documents...\n",
      "‚úÖ TF-IDF fitted in 0.70 seconds\n",
      "   ‚úì TF-IDF fitted successfully\n",
      "\n",
      "--- TRANSFORMING TRAINING DATA ---\n",
      "üîÑ Transforming 1000 texts with 1 extractors...\n",
      "   ‚úì TF-IDF: (1000, 5000), density=0.0970, 0.45s\n",
      "\n",
      "--- TRANSFORMING VALIDATION DATA ---\n",
      "üîÑ Transforming 200 texts with 1 extractors...\n",
      "   ‚úì TF-IDF: (200, 5000), density=0.0931, 0.09s\n",
      "\n",
      "‚úÖ Feature extraction completed for 1 extractors\n",
      "‚úÖ Feature extraction completed!\n",
      "üìä Results summary:\n",
      "   TF-IDF: Train(1000, 5000), Val(200, 5000), Density:0.0970\n"
     ]
    }
   ],
   "source": [
    "print(\"=== EXTRACTING FEATURES ===\")\n",
    "\n",
    "# Use smaller subset for initial testing\n",
    "train_subset = train_df.head(1000)  # Start with 1000 samples\n",
    "val_subset = val_df.head(200)       # Start with 200 samples\n",
    "\n",
    "print(f\"üìä Using subset for testing:\")\n",
    "print(f\"   Training: {len(train_subset)} samples\")\n",
    "print(f\"   Validation: {len(val_subset)} samples\")\n",
    "\n",
    "# Extract features using the pipeline\n",
    "try:\n",
    "    results = pipeline.extract_features(train_subset, val_subset)\n",
    "    \n",
    "    print(f\"‚úÖ Feature extraction completed!\")\n",
    "    print(f\"üìä Results summary:\")\n",
    "    for name, result in results.items():\n",
    "        train_shape = result['train']['shape']\n",
    "        val_shape = result['val']['shape']\n",
    "        density = result['train']['density']\n",
    "        print(f\"   {name}: Train{train_shape}, Val{val_shape}, Density:{density:.4f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Pipeline extraction failed: {e}\")\n",
    "    print(\"Let's debug step by step...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e84eb4d7-ac5e-43e1-ad81-5fd4b334d0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Available extractors: ['TF-IDF']\n",
      "\n",
      "üìà Feature Extraction Summary:\n",
      "  Extractor   Train_Shape  Features  Density  Memory_MB  Fit_Time_s  \\\n",
      "0    TF-IDF  (1000, 5000)      5000    0.097     3.7019      0.7025   \n",
      "\n",
      "   Transform_Time_s  \n",
      "0            0.4502  \n",
      "\n",
      "üîç TF-IDF Analysis:\n",
      "   Training shape: (1000, 5000)\n",
      "   Validation shape: (200, 5000)\n",
      "   Matrix density: 0.0970\n",
      "   Memory usage: 3.7 MB\n",
      "   Fit time: 0.70s\n",
      "   Transform time: 0.45s\n"
     ]
    }
   ],
   "source": [
    "# Check what extractors you have\n",
    "print(f\"üìä Available extractors: {list(results.keys())}\")\n",
    "\n",
    "# Get summary statistics\n",
    "summary = pipeline.get_summary()\n",
    "print(f\"\\nüìà Feature Extraction Summary:\")\n",
    "print(summary.round(4))\n",
    "\n",
    "# Analyze each extractor's results\n",
    "for extractor_name, result in results.items():\n",
    "    print(f\"\\nüîç {extractor_name} Analysis:\")\n",
    "    train_data = result['train']\n",
    "    val_data = result['val']\n",
    "    extractor = result['extractor']\n",
    "    \n",
    "    print(f\"   Training shape: {train_data['shape']}\")\n",
    "    print(f\"   Validation shape: {val_data['shape']}\")\n",
    "    print(f\"   Matrix density: {train_data['density']:.4f}\")\n",
    "    print(f\"   Memory usage: {train_data['memory_mb']:.1f} MB\")\n",
    "    print(f\"   Fit time: {extractor.fit_time:.2f}s\")\n",
    "    print(f\"   Transform time: {train_data['transform_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17993847-3a95-4b01-9cfc-cf69e4d8eaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREPARING DATA FOR MACHINE LEARNING ===\n",
      "‚úÖ TF-IDF ready for ML:\n",
      "   Training features: (1000, 5000)\n",
      "   Validation features: (200, 5000)\n",
      "\n",
      "‚úÖ Labels ready:\n",
      "   Training labels: 1000\n",
      "   Validation labels: 200\n",
      "   Label distribution: [226 527 247]\n",
      "\n",
      "üèÜ Best extractor (by density): TF-IDF\n",
      "‚úÖ Final ML variables created:\n",
      "   X_train: (1000, 5000)\n",
      "   X_val: (200, 5000)\n",
      "   y_train: 1000\n",
      "   y_val: 200\n"
     ]
    }
   ],
   "source": [
    "# Extract features and labels for ML training\n",
    "ml_ready_data = {}\n",
    "\n",
    "for extractor_name, result in results.items():\n",
    "    X_train_ext = result['train']['features']\n",
    "    X_val_ext = result['val']['features']\n",
    "    \n",
    "    ml_ready_data[extractor_name] = {\n",
    "        'X_train': X_train_ext,\n",
    "        'X_val': X_val_ext,\n",
    "        'train_shape': X_train_ext.shape,\n",
    "        'val_shape': X_val_ext.shape\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ {extractor_name} ready for ML:\")\n",
    "    print(f\"   Training features: {X_train_ext.shape}\")\n",
    "    print(f\"   Validation features: {X_val_ext.shape}\")\n",
    "\n",
    "# Get labels (same for all extractors)\n",
    "y_train = train_subset['labels'].values\n",
    "y_val = val_subset['labels'].values\n",
    "\n",
    "print(f\"\\n‚úÖ Labels ready:\")\n",
    "print(f\"   Training labels: {len(y_train)}\")\n",
    "print(f\"   Validation labels: {len(y_val)}\")\n",
    "print(f\"   Label distribution: {np.bincount(y_train)}\")\n",
    "\n",
    "# Choose best extractor for initial ML training\n",
    "summary = pipeline.get_summary()\n",
    "best_extractor = summary.loc[summary['Density'].idxmax(), 'Extractor']\n",
    "print(f\"\\nüèÜ Best extractor (by density): {best_extractor}\")\n",
    "\n",
    "# Set up final variables for ML training (NO GLOBALS NEEDED!)\n",
    "if best_extractor in ml_ready_data:\n",
    "    X_train = ml_ready_data[best_extractor]['X_train']\n",
    "    X_val = ml_ready_data[best_extractor]['X_val']\n",
    "    \n",
    "    print(f\"‚úÖ Final ML variables created:\")\n",
    "    print(f\"   X_train: {X_train.shape}\")\n",
    "    print(f\"   X_val: {X_val.shape}\")\n",
    "    print(f\"   y_train: {len(y_train)}\")\n",
    "    print(f\"   y_val: {len(y_val)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
