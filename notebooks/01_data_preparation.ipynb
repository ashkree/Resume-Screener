{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be7be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc450bdf-6484-46c8-af39-b78696ec5106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.TextPreprocessor import TextPreprocessor\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06325341",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3492953-3cc4-4392-b9e2-ff5bdbc0495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from hugging face \n",
    "# cnamuangtoun/resume-job-description-fit\n",
    "\n",
    "ds = load_dataset(\"cnamuangtoun/resume-job-description-fit\")\n",
    "train_df = ds['train'].to_pandas()\n",
    "test_df = ds['test'].to_pandas()\n",
    "\n",
    "# Create train/validation split\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.30,\n",
    "                                   stratify=train_df[\"label\"], random_state=42)\n",
    "\n",
    "def map_multiclass(dfs: List):\n",
    "\n",
    "    # Create label mapping\n",
    "    label_to_id = {\"Good Fit\": 0, \"No Fit\": 2, \"Potential Fit\":1}\n",
    "\n",
    "    for df in dfs:\n",
    "        df[\"label\"] = df[\"label\"].map(label_to_id)\n",
    "\n",
    "    return dfs[0], dfs[1], dfs[2]\n",
    "\n",
    "def map_binaryclass(dfs: List):\n",
    "\n",
    "    # Create label mapping\n",
    "    label_to_id = {\"Good Fit\": 0, \"No Fit\": 1, \"Potential Fit\":0}\n",
    "\n",
    "    for df in dfs:\n",
    "        df[\"label\"] = df[\"label\"].map(label_to_id)\n",
    "\n",
    "    return dfs[0], dfs[1], dfs[2]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def show_split_stats(train_df: pd.DataFrame,\n",
    "                     val_df: pd.DataFrame,\n",
    "                     test_df: pd.DataFrame,\n",
    "                     label_col: str = \"label\"):\n",
    "    header = (\n",
    "        f\"Data loaded and split:\\n\"\n",
    "        f\"  • Training:   {len(train_df):>6} samples\\n\"\n",
    "        f\"  • Validation: {len(val_df):>6} samples\\n\"\n",
    "        f\"  • Test:       {len(test_df):>6} samples\\n\"\n",
    "        f\"\\nTraining label distribution:\"\n",
    "    )\n",
    "\n",
    "    # Counts and percentages side-by-side\n",
    "    counts = train_df[label_col].value_counts(dropna=False)\n",
    "    pct = (counts / counts.sum() * 100).round(1)\n",
    "    stats = pd.concat([counts.rename(\"count\"), pct.rename(\"%\")], axis=1)\n",
    "\n",
    "    print(header)\n",
    "    print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b52113e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "— Binary —\n",
      "Data loaded and split:\n",
      "  • Training:     4368 samples\n",
      "  • Validation:   1873 samples\n",
      "  • Test:         1759 samples\n",
      "\n",
      "Training label distribution:\n",
      "       count     %\n",
      "label             \n",
      "1       2200  50.4\n",
      "0       2168  49.6\n",
      "\n",
      "— Multiclass —\n",
      "Data loaded and split:\n",
      "  • Training:     4368 samples\n",
      "  • Validation:   1873 samples\n",
      "  • Test:         1759 samples\n",
      "\n",
      "Training label distribution:\n",
      "       count     %\n",
      "label             \n",
      "2       2200  50.4\n",
      "1       1089  24.9\n",
      "0       1079  24.7\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Comparing multiclass dataset and binary dataset stats\n",
    "\"\"\"\n",
    "\n",
    "dfs = {\n",
    "    kind: dict(zip([\"train\", \"val\", \"test\"],\n",
    "                   func([train_df.copy(), val_df.copy(), test_df.copy()])))\n",
    "    for kind, func in {\n",
    "        \"binary\": map_binaryclass,\n",
    "        \"multiclass\": map_multiclass\n",
    "    }.items()\n",
    "}\n",
    "\n",
    "# ───────── quick sanity-check ─────────\n",
    "for kind, splits in dfs.items():\n",
    "    print(f\"\\n— {kind.capitalize()} —\")\n",
    "    show_split_stats(splits[\"train\"], splits[\"val\"], splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e913f7-7f0d-4f22-ac53-610d9f2b0d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_TEXT = True\n",
    "STOP_WORD_REMOVAL = False\n",
    "LEMMATIZE = False\n",
    "\n",
    "tp = TextPreprocessor(enable_stopwords=STOP_WORD_REMOVAL, enable_lemmatizer=LEMMATIZE)\n",
    "\n",
    "for kind, splits in dfs.items():\n",
    "    for split_name, df in splits.items():\n",
    "        dfs[kind][split_name] = tp.process_dataset(df, clean_text=CLEAN_TEXT, remove_stop_words=STOP_WORD_REMOVAL, lemmatize=LEMMATIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6dd2a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SkillNER features...\n",
      "Loading spaCy model: en_core_web_md\n",
      "loading full_matcher ...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__call__() got an unexpected keyword argument 'attr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Preprocess with SkillNER (this takes time but only done once)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m SkillNERPreprocessor(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_md\u001b[39m\u001b[38;5;124m\"\u001b[39m, cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../skillner_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m df_with_skills \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mpreprocess_data(df)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m kind, splits \u001b[38;5;129;01min\u001b[39;00m dfs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m split_name, df \u001b[38;5;129;01min\u001b[39;00m splits\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/Projects/Resume-Screener/notebooks/../utils/SkillNERPreprocessor.py:114\u001b[0m, in \u001b[0;36mSkillNERPreprocessor.preprocess_data\u001b[0;34m(self, df, force_recompute)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing SkillNER features...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_extractor()\n\u001b[1;32m    116\u001b[0m     skill_features \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    117\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Projects/Resume-Screener/notebooks/../utils/SkillNERPreprocessor.py:32\u001b[0m, in \u001b[0;36mSkillNERPreprocessor._init_extractor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading spaCy model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskill_extractor \u001b[38;5;241m=\u001b[39m SkillExtractor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp, SKILL_DB, PhraseMatcher(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp\u001b[38;5;241m.\u001b[39mvocab))\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkillNER extractor initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/304/lib/python3.12/site-packages/skillNer/skill_extractor_class.py:52\u001b[0m, in \u001b[0;36mSkillExtractor.__init__\u001b[0;34m(self, nlp, skills_db, phraseMatcher, tranlsator_func)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphraseMatcher \u001b[38;5;241m=\u001b[39m phraseMatcher\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# load matchers: all\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatchers \u001b[38;5;241m=\u001b[39m Matchers(\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp,\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskills_db,\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphraseMatcher,\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# self.stop_words\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m )\u001b[38;5;241m.\u001b[39mload_matchers()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# init skill getters\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskill_getters \u001b[38;5;241m=\u001b[39m SkillsGetter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp)\n",
      "File \u001b[0;32m~/.conda/envs/304/lib/python3.12/site-packages/skillNer/matcher_class.py:108\u001b[0m, in \u001b[0;36mMatchers.load_matchers\u001b[0;34m(self, include, exclude)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m matcher_name \u001b[38;5;129;01min\u001b[39;00m include:\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmatcher_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 108\u001b[0m             loaded_matchers[matcher_name] \u001b[38;5;241m=\u001b[39m matcher()\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_matchers\n",
      "File \u001b[0;32m~/.conda/envs/304/lib/python3.12/site-packages/skillNer/matcher_class.py:118\u001b[0m, in \u001b[0;36mMatchers.get_full_matcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp\n\u001b[1;32m    117\u001b[0m skills_db \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskills_db\n\u001b[0;32m--> 118\u001b[0m full_matcher \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphraseMatcher(nlp\u001b[38;5;241m.\u001b[39mvocab, attr\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOWER\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# populate matcher\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m skills_db:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# get skill info\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/304/lib/python3.12/site-packages/spacy/matcher/phrasematcher.pyx:236\u001b[0m, in \u001b[0;36mspacy.matcher.phrasematcher.PhraseMatcher.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() got an unexpected keyword argument 'attr'"
     ]
    }
   ],
   "source": [
    "from utils import SkillNERPreprocessor\n",
    "\n",
    "# Preprocess with SkillNER (this takes time but only done once)\n",
    "preprocessor = SkillNERPreprocessor(model=\"en_core_web_md\", cache_dir=\"../skillner_cache\")\n",
    "df_with_skills = preprocessor.preprocess_data(df)\n",
    "\n",
    "for kind, splits in dfs.items():\n",
    "    for split_name, df in splits.items():\n",
    "        dfs[kind][split_name] = tp.process_dataset(df, clean_text=CLEAN_TEXT, remove_stop_words=STOP_WORD_REMOVAL, lemmatize=LEMMATIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e4a2c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🗂  Exporting binary dataset → ../data/full_process/undersampled/binary  (strategy=undersample)\n",
      "  train  | before: {1: 2200, 0: 2168}  →  after: {1: 2168, 0: 2168}\n",
      "  val    | before: {0: 930, 1: 943}  →  after: {0: 930, 1: 930}\n",
      "  test   | before: {1: 857, 0: 902}  →  after: {1: 857, 0: 857}\n",
      "\n",
      "🗂  Exporting multiclass dataset → ../data/full_process/undersampled/multiclass  (strategy=undersample)\n",
      "  train  | before: {2: 2200, 1: 1089, 0: 1079}  →  after: {0: 1079, 2: 1079, 1: 1079}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jp/dk7s9_397rs1z_gzx3b_z4380000gn/T/ipykernel_21009/2425218523.py:22: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(target, replace=False, random_state=SEED))\n",
      "/var/folders/jp/dk7s9_397rs1z_gzx3b_z4380000gn/T/ipykernel_21009/2425218523.py:22: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(target, replace=False, random_state=SEED))\n",
      "/var/folders/jp/dk7s9_397rs1z_gzx3b_z4380000gn/T/ipykernel_21009/2425218523.py:22: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(target, replace=False, random_state=SEED))\n",
      "/var/folders/jp/dk7s9_397rs1z_gzx3b_z4380000gn/T/ipykernel_21009/2425218523.py:50: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
      "/var/folders/jp/dk7s9_397rs1z_gzx3b_z4380000gn/T/ipykernel_21009/2425218523.py:22: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(target, replace=False, random_state=SEED))\n",
      "/var/folders/jp/dk7s9_397rs1z_gzx3b_z4380000gn/T/ipykernel_21009/2425218523.py:22: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(target, replace=False, random_state=SEED))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val    | before: {0: 463, 2: 943, 1: 467}  →  after: {1: 463, 0: 463, 2: 463}\n",
      "  test   | before: {2: 857, 1: 444, 0: 458}  →  after: {2: 444, 0: 444, 1: 444}\n",
      "\n",
      "✅ All balanced splits saved to: /Users/mave/Documents/UniFiles/Resume-Screener/data/full_process/undersampled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jp/dk7s9_397rs1z_gzx3b_z4380000gn/T/ipykernel_21009/2425218523.py:22: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(target, replace=False, random_state=SEED))\n",
      "/var/folders/jp/dk7s9_397rs1z_gzx3b_z4380000gn/T/ipykernel_21009/2425218523.py:50: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    }
   ],
   "source": [
    "import os, pickle, json, pathlib, random\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "DEST_DIR         = pathlib.Path(\"../data/full_process/undersampled\")\n",
    "BALANCE_STRATEGY = \"undersample\"        # \"oversample\" | \"undersample\"\n",
    "SEED             = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "def balance_df(df, label_col=\"label\", strategy=\"oversample\"):\n",
    "    counts = df[label_col].value_counts()\n",
    "    if strategy == \"oversample\":\n",
    "        target = counts.max()\n",
    "        balanced = (\n",
    "            df.groupby(label_col, group_keys=False)\n",
    "              .apply(lambda g: g.sample(target, replace=True, random_state=SEED))\n",
    "        )\n",
    "    elif strategy == \"undersample\":\n",
    "        target = counts.min()\n",
    "        balanced = (\n",
    "            df.groupby(label_col, group_keys=False)\n",
    "              .apply(lambda g: g.sample(target, replace=False, random_state=SEED))\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"strategy must be 'oversample' or 'undersample'\")\n",
    "    return balanced.sample(frac=1, random_state=SEED)\n",
    "\n",
    "for kind, splits in dfs.items():\n",
    "    out_dir = DEST_DIR / kind\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n🗂  Exporting {kind} dataset → {out_dir}  (strategy={BALANCE_STRATEGY})\")\n",
    "\n",
    "    for split_name, df in splits.items():\n",
    "        before = Counter(df[\"label\"])\n",
    "        df_bal = balance_df(df, label_col=\"label\", strategy=BALANCE_STRATEGY)\n",
    "        after  = Counter(df_bal[\"label\"])\n",
    "\n",
    "        print(f\"  {split_name:<5}  | before: {dict(before)}  →  after: {dict(after)}\")\n",
    "\n",
    "        X = df_bal.drop(columns=[\"label\"])\n",
    "        y = df_bal[\"label\"]\n",
    "\n",
    "        with open(out_dir / f\"X_{split_name}.pkl\", \"wb\") as fx:\n",
    "            pickle.dump(X, fx, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(out_dir / f\"y_{split_name}.pkl\", \"wb\") as fy:\n",
    "            pickle.dump(y, fy, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    manifest = {\n",
    "        \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"balance_strategy\": BALANCE_STRATEGY,\n",
    "        \"seed\": SEED,\n",
    "        \"files\": sorted([p.name for p in out_dir.glob('*.pkl')]),\n",
    "    }\n",
    "    with open(out_dir / \"manifest.json\", \"w\") as mf:\n",
    "        json.dump(manifest, mf, indent=2)\n",
    "\n",
    "print(\"\\n✅ All balanced splits saved to:\", DEST_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c846a53a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "304",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
